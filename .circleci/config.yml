version: 2
jobs:
  # Run Lighthouse against staging deployment -- anonymous user hitting 
  # the app's home page
  perfTests:
    # Number of parallel Lighthouse runs against this url. Why more than one?
    # Some perf metrics vary across runs based on backend flakiness, etc, and 
    # this way we can extract a best score or median across runs.
    parallelism: 2

    # Sadly there is currently no official lighthouse docker image. But
    # it is on their radar github.com/GoogleChrome/lighthouse/issues/3715
    docker:
      - image: kanopi/ci:edge-lghths

    steps:
      - checkout

      - run:
          name: Run lighthouse against staging deployment

          # - Extract max JS bundle size, as well as a regular expression that
          #   identifies our main JS bundle, from their definitions in `package.json`.
          #   Store them in environment variables so that our custom confg
          #   can reference them.
          #
          # - Invoke `lighthouse`, passing in a custom config file via --config-path.
          #   The config file is how we include our custom audit.
          #
          # - The container is defined to run as user `chrome` (side note, can't run
          #   Chrome browser as root!) We store the Lighthouse reports in user 
          #   `chrome`'s home directory via the --output-path.
          #
          # - Also via --output-path, the reports will have in their file 
          #   name `anonymous` (as opposed to authenticated) and a checksum 
          #   of a unique value from the container where they were executed 
          #
          # - We use --output to get reports in html, so people can view them, 
          #   AND json, so we can extract scores from them programmatically.
          command: |
            TEST_URL="$(node -p 'require("./lighthouse.json").url')" \
            lighthouse $TEST_URL \
              --port=9222 \
              --chrome-flags=\"--headless\" \
              --output-path=/opt/reports/anonymous-"$(echo -n $CIRCLE_SHELL_ENV | md5sum | awk '{print $1}')" \
              --output=json \
              --output=html

      # Save the reports just generated in a place where the _next job_ in the
      # workflow can snag them all and do analysis on them. 
      - persist_to_workspace:
          root: /opt
          paths:
            - reports


  # Analyze all the reports, decide if we should pass or fail, and
  # report back with a comment on the PR that provides A) the scores and
  # B) links to the html reports for all the test runs.
  processResults:
    docker:
      - image: kanopi/ci:edge-lghths

    steps:
      - checkout



      # Mount the workspace (which contains all our reports) into this
      # container. The reports are subsequently available at ./reports/
      - attach_workspace:
          at: "."

      # Store the html and json reports in S3 as long-term artifacts associated
      # with this job. Then, we can easily send links to the html reports in the
      # PR comment.
      - store_artifacts:
          path: reports
          destination: reports

      # Compare our desired goals, expressed in the `lighthouse` section of
      # `package.json`, against the actual test results. Right now we simply
      # use the _best score_ across all runs for each category. If we want
      # to fail the job, and consequently fail the workflow, the script exits
      # with a nonzero exit code.
      #
      # Finally, we post a comment to GH. Because we enabled GH "checks integration"
      # on the CircleCI side, the PR status will automatically be set to
      # pass/fail based on whether this workflow passes or fails.
      - run:
          shell: /bin/sh
          name: Analyze and report desired vs actual lighthouse scores
          command: |
            /opt/ci-scripts/analyze_scores.js lighthouse.json reports



workflows:
  version: 2
  deployToStagingAndTest:
    jobs:
      - perfTests
      - processResults:
          requires:
            - perfTests
